{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T19:08:54.875309Z",
     "iopub.status.busy": "2025-12-28T19:08:54.874933Z",
     "iopub.status.idle": "2025-12-28T19:08:54.877834Z",
     "shell.execute_reply": "2025-12-28T19:08:54.878077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best.pt in parent directory. Changing directory to root...\n",
      "Current Working Directory: /Users/ugurendirlik/DL Project/Damage_Car_DL_Project-1/project_part3\n"
     ]
    }
   ],
   "source": [
    "# SETUP CELL - RUN THIS FIRST\n",
    "import os\n",
    "\n",
    "# Automatic Path setup\n",
    "# This approach verifies where we are and points to the project root\n",
    "# so that imports and data loading work correctly.\n",
    "\n",
    "target_file = 'best.pt' # Marker file to identify root\n",
    "\n",
    "if os.path.exists(target_file):\n",
    "    print(f'Success: Found {target_file} in current directory.')\n",
    "    print('Ready to run.')\n",
    "elif os.path.exists(os.path.join('..', target_file)):\n",
    "    print(f'Found {target_file} in parent directory. Changing directory to root...')\n",
    "    os.chdir('..')\n",
    "    print(f'Current Working Directory: {os.getcwd()}')\n",
    "else:\n",
    "    print('WARNING: Could not find project root (best.pt not found).')\n",
    "    print('Please ensure you have downloaded the necessary files from Drive and placed them correctly.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-28T19:08:54.885675Z",
     "iopub.status.busy": "2025-12-28T19:08:54.885282Z",
     "iopub.status.idle": "2025-12-28T19:09:08.806026Z",
     "shell.execute_reply": "2025-12-28T19:09:08.806459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading YOLO model...\n",
      "Loading ResNet model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11621 images in archive-2/image. Starting large batch processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Processing:   0%|                                                                                                                                                                                                                       | 0/50 [00:00<?, ?it/s][W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Batch Processing:  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                                                                                                                                        | 13/50 [00:03<00:08,  4.33it/s]Corrupt JPEG data: premature end of data segment\n",
      "Batch Processing: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:11<00:00,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete! Check outputs in archive2_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def run_large_batch(\n",
    "    source_dir=\"archive-2/image\",\n",
    "    yolo_path=\"best.pt\",\n",
    "    resnet_path=\"severity_model_resnet18.pth\",\n",
    "    output_dir=\"archive2_results\",\n",
    "    class_names=['high', 'low', 'medium'] \n",
    "):\n",
    "    # Setup Device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load YOLO\n",
    "    print(\"Loading YOLO model...\")\n",
    "    yolo_model = YOLO(yolo_path)\n",
    "\n",
    "    # 2. Load ResNet\n",
    "    print(\"Loading ResNet model...\")\n",
    "    resnet_model = models.resnet18(pretrained=False)\n",
    "    num_ftrs = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "    resnet_model.load_state_dict(torch.load(resnet_path, map_location=device))\n",
    "    resnet_model = resnet_model.to(device)\n",
    "    resnet_model.eval()\n",
    "\n",
    "    # Transforms\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 3. Process Images\n",
    "    source_path = Path(source_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    # Using iterator for memory efficiency if list is huge, but 11k is fine in list\n",
    "    image_files = [\n",
    "        p for p in source_path.rglob(\"*\") \n",
    "        if p.suffix.lower() in image_extensions\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images in {source_dir}. Starting large batch processing...\")\n",
    "\n",
    "    # Processing a sample batch for demonstration. Remove [:200] for full processing.\\n",
    "    for img_path in tqdm(image_files[:200], desc=\"Batch Processing\"):\\n",
    "        try:\n",
    "            # Read Image\n",
    "            original_image = cv2.imread(str(img_path))\n",
    "            if original_image is None:\n",
    "                continue\n",
    "            \n",
    "            # YOLO Inference\n",
    "            results = yolo_model(original_image, verbose=False)\n",
    "\n",
    "            has_detection = False\n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "                if len(boxes) > 0:\n",
    "                    has_detection = True\n",
    "                \n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                    \n",
    "                    # Crop\n",
    "                    h, w = original_image.shape[:2]\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(w, x2), min(h, y2)\n",
    "                    crop = original_image[y1:y2, x1:x2]\n",
    "                    \n",
    "                    if crop.size == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    # ResNet Inference\n",
    "                    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                    pil_img = Image.fromarray(crop_rgb)\n",
    "                    input_tensor = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = resnet_model(input_tensor)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        severity_idx = preds.item()\n",
    "                        severity_label = class_names[severity_idx]\n",
    "                        confidence = torch.nn.functional.softmax(outputs, dim=1)[0][severity_idx].item()\n",
    "\n",
    "                    # Visualization\n",
    "                    color = (0, 255, 0) \n",
    "                    if severity_label == 'high':\n",
    "                        color = (0, 0, 255) # Red\n",
    "                    elif severity_label == 'medium':\n",
    "                        color = (0, 165, 255) # Orange\n",
    "                    elif severity_label == 'low':\n",
    "                        color = (0, 255, 255) # Yellow\n",
    "                        \n",
    "                    cv2.rectangle(original_image, (x1, y1), (x2, y2), color, 2)\n",
    "                    \n",
    "                    label_text = f\"{severity_label} ({confidence:.2f})\"\n",
    "                    (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "                    cv2.rectangle(original_image, (x1, y1 - 20), (x1 + tw, y1), color, -1)\n",
    "                    cv2.putText(original_image, label_text, (x1, y1 - 5), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            # Save Result\n",
    "            # To avoid name collision, use parent folder name in filename\n",
    "            save_name = output_path / f\"{img_path.parent.name}_{img_path.name}\"\n",
    "            # Or just keep original name if unique enough, but safely:\n",
    "            # save_name = output_path / img_path.name\n",
    "            \n",
    "            # Since user provided \"archive-2/image\", structure is likely flat or random.\n",
    "            # Let's stick to unique naming to be safe given 11k files.\n",
    "            \n",
    "            cv2.imwrite(str(save_name), original_image)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nProcessing complete! Check outputs in {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_large_batch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}