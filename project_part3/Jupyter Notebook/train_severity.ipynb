{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP CELL - RUN THIS FIRST\n",
    "import os\n",
    "\n",
    "# Automatic Path setup\n",
    "# This approach verifies where we are and points to the project root\n",
    "# so that imports and data loading work correctly.\n",
    "\n",
    "target_file = 'best.pt' # Marker file to identify root\n",
    "\n",
    "if os.path.exists(target_file):\n",
    "    print(f'Success: Found {target_file} in current directory.')\n",
    "    print('Ready to run.')\n",
    "elif os.path.exists(os.path.join('..', target_file)):\n",
    "    print(f'Found {target_file} in parent directory. Changing directory to root...')\n",
    "    os.chdir('..')\n",
    "    print(f'Current Working Directory: {os.getcwd()}')\n",
    "else:\n",
    "    print('WARNING: Could not find project root (best.pt not found).')\n",
    "    print('Please ensure you have downloaded the necessary files from Drive and placed them correctly.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def train_model():\n",
    "    # Configuration\n",
    "    DATA_DIR = 'severity_data'\n",
    "    MODEL_SAVE_PATH = 'severity_model_resnet18.pth'\n",
    "    NUM_CLASSES = 3  # low, medium, high\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 15\n",
    "    LEARNING_RATE = 0.001\n",
    "    DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # Use M2 GPU (MPS) if available\n",
    "    \n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # Data Transforms\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    # Load Dataset\n",
    "    full_dataset = datasets.ImageFolder(DATA_DIR, transform=data_transforms['train'])\n",
    "    class_names = full_dataset.classes\n",
    "    print(f\"Classes: {class_names}\")\n",
    "\n",
    "    # Split Train/Val (80/20)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    # Apply 'val' transform to validation dataset (hacky way because random_split doesn't support different transforms)\n",
    "    # We will just accept that val set has some augmentation or strictly reload if needed. \n",
    "    # For simplicity/speed here, we use the same dataset object. \n",
    "    # Ideally we'd wrap it to override transform, but for this project data augmentation on val (except resize) is minimal\n",
    "    # actually, ImageFolder applies transform at loading time.\n",
    "    # To do it properly: create two dataset objects pointing to same folder, one with train transform, one with val transform,\n",
    "    # then split indices.\n",
    "    \n",
    "    # Proper Reset for transforms\n",
    "    train_dataset.dataset.transform = data_transforms['train']\n",
    "    # Create a new validation dataset with correct transform\n",
    "    # We can't easily deepcopy just the transform of a Subset, so let's stick to the simple split \n",
    "    # and just remember validation metrics will be slightly \"harder\" due to augmentation or we ignore it for now.\n",
    "    # Given the small dataset, data augmentation on validation isn't fatal, but let's correct it for best practice.\n",
    "    \n",
    "    # Reloading specifically for clean split logic\n",
    "    full_data_train = datasets.ImageFolder(DATA_DIR, transform=data_transforms['train'])\n",
    "    full_data_val = datasets.ImageFolder(DATA_DIR, transform=data_transforms['val'])\n",
    "    \n",
    "    # Use consistent seed for splitting indices\n",
    "    torch.manual_seed(42)\n",
    "    indices = torch.randperm(len(full_data_train)).tolist()\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    train_subset = torch.utils.data.Subset(full_data_train, train_indices)\n",
    "    val_subset = torch.utils.data.Subset(full_data_val, val_indices)\n",
    "\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True),\n",
    "        'val': DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    }\n",
    "    dataset_sizes = {'train': len(train_subset), 'val': len(val_subset)}\n",
    "    print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "\n",
    "    # Initialize Model (ResNet18)\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # Training Loop\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f'Epoch {epoch}/{NUM_EPOCHS - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            # Fix for MPS: Use .float() instead of .double() or cast result to cpu first\n",
    "            epoch_acc = running_corrects.float() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc.item())\n",
    "            else:\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    # Evaluate on Validation Set\n",
    "    print(\"\\nEvaluating on Validation Set (Detailed Report)...\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders['val']:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Classification Report\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - Validation Set')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    print(\"Confusion matrix saved as 'confusion_matrix.png'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}