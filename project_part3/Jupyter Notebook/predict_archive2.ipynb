{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP CELL - RUN THIS FIRST\n",
    "import os\n",
    "\n",
    "# Automatic Path setup\n",
    "# This approach verifies where we are and points to the project root\n",
    "# so that imports and data loading work correctly.\n",
    "\n",
    "target_file = 'best.pt' # Marker file to identify root\n",
    "\n",
    "if os.path.exists(target_file):\n",
    "    print(f'Success: Found {target_file} in current directory.')\n",
    "    print('Ready to run.')\n",
    "elif os.path.exists(os.path.join('..', target_file)):\n",
    "    print(f'Found {target_file} in parent directory. Changing directory to root...')\n",
    "    os.chdir('..')\n",
    "    print(f'Current Working Directory: {os.getcwd()}')\n",
    "else:\n",
    "    print('WARNING: Could not find project root (best.pt not found).')\n",
    "    print('Please ensure you have downloaded the necessary files from Drive and placed them correctly.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def run_large_batch(\n",
    "    source_dir=\"archive-2/image\",\n",
    "    yolo_path=\"best.pt\",\n",
    "    resnet_path=\"severity_model_resnet18.pth\",\n",
    "    output_dir=\"archive2_results\",\n",
    "    class_names=['high', 'low', 'medium'] \n",
    "):\n",
    "    # Setup Device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 1. Load YOLO\n",
    "    print(\"Loading YOLO model...\")\n",
    "    yolo_model = YOLO(yolo_path)\n",
    "\n",
    "    # 2. Load ResNet\n",
    "    print(\"Loading ResNet model...\")\n",
    "    resnet_model = models.resnet18(pretrained=False)\n",
    "    num_ftrs = resnet_model.fc.in_features\n",
    "    resnet_model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "    resnet_model.load_state_dict(torch.load(resnet_path, map_location=device))\n",
    "    resnet_model = resnet_model.to(device)\n",
    "    resnet_model.eval()\n",
    "\n",
    "    # Transforms\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # 3. Process Images\n",
    "    source_path = Path(source_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "    # Using iterator for memory efficiency if list is huge, but 11k is fine in list\n",
    "    image_files = [\n",
    "        p for p in source_path.rglob(\"*\") \n",
    "        if p.suffix.lower() in image_extensions\n",
    "    ]\n",
    "\n",
    "    print(f\"Found {len(image_files)} images in {source_dir}. Starting large batch processing...\")\n",
    "\n",
    "    for img_path in tqdm(image_files, desc=\"Batch Processing\"):\n",
    "        try:\n",
    "            # Read Image\n",
    "            original_image = cv2.imread(str(img_path))\n",
    "            if original_image is None:\n",
    "                continue\n",
    "            \n",
    "            # YOLO Inference\n",
    "            results = yolo_model(original_image, verbose=False)\n",
    "\n",
    "            has_detection = False\n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "                if len(boxes) > 0:\n",
    "                    has_detection = True\n",
    "                \n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "                    \n",
    "                    # Crop\n",
    "                    h, w = original_image.shape[:2]\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(w, x2), min(h, y2)\n",
    "                    crop = original_image[y1:y2, x1:x2]\n",
    "                    \n",
    "                    if crop.size == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    # ResNet Inference\n",
    "                    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "                    pil_img = Image.fromarray(crop_rgb)\n",
    "                    input_tensor = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = resnet_model(input_tensor)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        severity_idx = preds.item()\n",
    "                        severity_label = class_names[severity_idx]\n",
    "                        confidence = torch.nn.functional.softmax(outputs, dim=1)[0][severity_idx].item()\n",
    "\n",
    "                    # Visualization\n",
    "                    color = (0, 255, 0) \n",
    "                    if severity_label == 'high':\n",
    "                        color = (0, 0, 255) # Red\n",
    "                    elif severity_label == 'medium':\n",
    "                        color = (0, 165, 255) # Orange\n",
    "                    elif severity_label == 'low':\n",
    "                        color = (0, 255, 255) # Yellow\n",
    "                        \n",
    "                    cv2.rectangle(original_image, (x1, y1), (x2, y2), color, 2)\n",
    "                    \n",
    "                    label_text = f\"{severity_label} ({confidence:.2f})\"\n",
    "                    (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "                    cv2.rectangle(original_image, (x1, y1 - 20), (x1 + tw, y1), color, -1)\n",
    "                    cv2.putText(original_image, label_text, (x1, y1 - 5), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "\n",
    "            # Save Result\n",
    "            # To avoid name collision, use parent folder name in filename\n",
    "            save_name = output_path / f\"{img_path.parent.name}_{img_path.name}\"\n",
    "            # Or just keep original name if unique enough, but safely:\n",
    "            # save_name = output_path / img_path.name\n",
    "            \n",
    "            # Since user provided \"archive-2/image\", structure is likely flat or random.\n",
    "            # Let's stick to unique naming to be safe given 11k files.\n",
    "            \n",
    "            cv2.imwrite(str(save_name), original_image)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nProcessing complete! Check outputs in {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_large_batch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}